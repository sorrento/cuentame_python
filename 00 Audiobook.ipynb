{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import seleccion_txt, txt_read, get_fakes, cabeza_y_cola, corta\n",
    "from ut.textmining import get_word_matrix\n",
    "\n",
    "PATH_CALIBRE = 'c:/Users/milen/Biblioteca de calibre/'\n",
    "lang = \"ES\"  # >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 libros de referencia para hacer el tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'freakonomics'  # keyword para seleccionar el libro\n",
    "\n",
    "last, all_ = seleccion_txt(PATH_CALIBRE)  # trae todoslos título de libros en español\n",
    "# get the filename with path from all_ list\n",
    "file = [x for x in all_ if key in x.lower()][0]  # elejimos el primero\n",
    "\n",
    "date_es = 20220703 if lang == 'EN' else 20200504\n",
    "files_es, _ = seleccion_txt(PATH_CALIBRE, fecha=date_es)\n",
    "files = [file] + files_es  # usamos libros para generar el corpus para el tfidf\n",
    "doc_list = [txt_read(x) for x in files]  # lee los archivos y los pone en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf para los fakenames\n",
    "vector_matrix, vocab, _ = get_word_matrix(doc_list)\n",
    "dic_fake, di_counts = get_fakes(doc_list, files, vector_matrix, vocab, lang, openAI=True)\n",
    "dic_fake = {0: dic_fake[0]}\n",
    "pd.DataFrame.from_dict(dic_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_book = dic_fake[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Imagen\n",
    "Se lee desde Calibre, luego se corta manualmente y se guarda en dos tamaños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ipywidgets import fixed, interactive\n",
    "from ut.images import crop\n",
    "from utils import get_books, get_image_path, upload_lib_summary, get_book_datas\n",
    "from ut.io import get_filename\n",
    "from ut.base import json_read\n",
    "\n",
    "PATH_CALIBRE = 'c:/Users/milen/Biblioteca de calibre/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = get_image_path(file)\n",
    "titulo = get_filename(file, True).split(' - ')[0]\n",
    "print(titulo)  # ojo que puede estar cortado por Calire si es muy largo... quizas deberíamos cogerlo de la carpeta..\n",
    "im = Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if im.size[0] > 700:\n",
    "    im = im.reduce(2)\n",
    "im.reduce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = interactive(crop, f=(0.1, 1, 0.05),\n",
    "                sx=(1, int(im.size[0] * .5)),\n",
    "                sy=(1, int(im.size[1] * .9)),\n",
    "                img=fixed(im))\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = u.result.size[0]\n",
    "a = 200\n",
    "b = min(si, 2 * a)\n",
    "im_low = u.result.resize((a, a))\n",
    "im_hi = u.result.resize((b, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'data_out/_images/{}/{}.jpg'\n",
    "im_low.save(base.format('low', titulo))\n",
    "im_hi.save(base.format('hi', titulo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir en partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dic_fake[0]['path']\n",
    "texto = txt_read(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te qudas con la i del lo que parece el final del libro\n",
    "partes, df = cabeza_y_cola(texto, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = 802  # >>>\n",
    "ini = 15  # >>>\n",
    "\n",
    "partes, df = corta(partes, df, ini, fin)\n",
    "df = df.reset_index().rename(columns={'index': 'i'})\n",
    "df['ii'] = 0  # para identificar dentro de un párrafo largo que romperemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from ut.base import json_read, json_save, make_folder, json_update\n",
    "from utils import crea_capsulas_max, get_parrafos, get_final_parrfs, speakers_test, get_df_capitulos, \\\n",
    "    get_dic_capitulos, update_di_capi, procesa_capitulo, get_book_datas, SUMMARIES_JSON, sample_speaker, test_voices_en, \\\n",
    "    CONTENT_JSON\n",
    "from ut.textmining import palabras_representativas\n",
    "\n",
    "LIM = 850  # largo de las cápsulas, límite de lo que puede leer el sinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final, partes = get_final_parrfs(df, LIM)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_capsulas = crea_capsulas_max(partes, final, lmax=LIM, verbose=False)\n",
    "caps = ['.\\n'.join(d_capsulas[x]['texto']) for x in d_capsulas]  # todo probar si sintetizador lee punto aparte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps[12]  # las cápsulas son las que puede leer de una sola vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupamos las cápsulas en capítulos (par que sean 25)\n",
    "df_capitulos = get_df_capitulos(caps)\n",
    "df_capitulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_capitulos = get_dic_capitulos(df_capitulos)\n",
    "d_capitulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista con el texto total del capítulo\n",
    "capitulos = ['\\n '.join(d_capitulos[cap]['capsulas']) for cap in d_capitulos]\n",
    "capitulos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitulos_titles = palabras_representativas(capitulos,\n",
    "                                            n_best=20,  # 15\n",
    "                                            n_pick=20,  # no se usa si sorted is true\n",
    "                                            sorted=True,\n",
    "                                            l_exclude=dict_book['names'],\n",
    "                                            max_df=.4,  # .8  proporción de documentos. si lo bajamos quitamos los muy frecuentes\n",
    "                                            min_df=.2)  # .2  % de docs. Si lo subo quito palabras poco frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitulos_titles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import genera_titulo_openAI\n",
    "titles_openai = []\n",
    "for di_cap in capitulos_titles:\n",
    "    title = genera_titulo_openAI(di_cap)\n",
    "    print(title)\n",
    "    titles_openai.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ponemos los títulos en el diccionario de capítulos\n",
    "for i, cap in enumerate(d_capitulos):\n",
    "    # el primer capítulo es 1\n",
    "    d_capitulos[i+1]['title'] = titles_openai[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_capitulos[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `d_capitulos` tiene 25 capítulos, son los equivalentes a una canción cuando hagamos el mp3\n",
    "- Dentro están las `cápsulas` que son las que puede leer en cada vez. Haremos un archivo con cada uno y luego los uniremos\n",
    "Son los **capitulos** por lo tanto los que tienen que tener un nombre (canción)\n",
    "También está `title`, que es el título del capitulo generado por chatgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_di_capi(d_capitulos, titles_openai, d_summary, titulo)\n",
    "path_book = make_folder('data_out/' + titulo + '/')\n",
    "json_save(d_capitulos, path_book + CONTENT_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Preguntas económicas en el escenario criminal de 2003?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ut.base import json_read\n",
    "from utils import CONTENT_JSON\n",
    "path_book ='data_out/Freakonomics/'\n",
    "d_capitulos = json_read(path_book + CONTENT_JSON, keys_as_integer=True)\n",
    "d_capitulos[1]['song']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "torch.hub.download_url_to_file('https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml',\n",
    "                               'latest_silero_models.yml',\n",
    "                               progress=False)\n",
    "\n",
    "models = OmegaConf.load('latest_silero_models.yml')\n",
    "available_languages = list(models.tts_models.keys())\n",
    "\n",
    "for lang in available_languages:\n",
    "    modeli = list(models.tts_models.get(lang).keys())\n",
    "    print(f'Available models for {lang}: {modeli}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuración\n",
    "language = d_summary['idioma'].lower()\n",
    "model_id = 'v3_es' if language == 'es' else 'v3_en'\n",
    "\n",
    "sample_rate = 48000\n",
    "put_accent = True\n",
    "put_yo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el modelo\n",
    "device = torch.device('cpu')  # or cuda, pero no me funciona\n",
    "\n",
    "model, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                     model='silero_tts',\n",
    "                                     language=language,\n",
    "                                     speaker=model_id)\n",
    "model.to(device)  # gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps = [x for x in model.speakers if x != 'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_capitulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the keys to int\n",
    "d_capitulos = {int(k): v for k, v in d_capitulos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import speakers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language == 'es':\n",
    "    speakers_test(model,\n",
    "                  txt=d_capitulos[1]['capsulas'][0][:450],\n",
    "                  lan='es'\n",
    "                  #                  txt='Millonarios por una semana.\\n Cuando no se tiene una chaucha en el bolsillo, no es muy amplia la gama de actividades elegibles para matar el tiempo. Con Diego y Vittorio nos juntábamos casi todos los d'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
